{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# IBM Applied Data Science Capstone\n",
        "## Part 5: Predictive Analysis - Classification\n",
        "\n",
        "**Objective:** Build classification models to predict First Stage Landing Success\n",
        "\n",
        "**Author:** Son Nguyen\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "PREDICTIVE ANALYSIS: First Stage Landing Success Prediction\n",
            "============================================================\n",
            "\n",
            "‚úì Dataset loaded: (187, 23)\n",
            "‚úì Launches with landing attempts: 158\n",
            "‚úì Landing Success rate: 90.5%\n",
            "‚úì Target variable distribution:\n",
            "  - Successful Landings: 143 (90.5%)\n",
            "  - Failed Landings: 15 (9.5%)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set visualization style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"PREDICTIVE ANALYSIS: First Stage Landing Success Prediction\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Load cleaned SpaceX dataset (already has all engineered features)\n",
        "df = pd.read_csv('../data/spacex_launches_cleaned.csv')\n",
        "\n",
        "# Filter only launches with landing attempts\n",
        "df_model = df[df['Core_Landing'] != 'No Attempt'].copy()\n",
        "\n",
        "print(f\"\\n‚úì Dataset loaded: {df.shape}\")\n",
        "print(f\"‚úì Launches with landing attempts: {len(df_model)}\")\n",
        "print(f\"‚úì Landing Success rate: {df_model['Landing_Success'].mean()*100:.1f}%\")\n",
        "print(f\"‚úì Target variable distribution:\")\n",
        "print(f\"  - Successful Landings: {df_model['Landing_Success'].sum()} ({df_model['Landing_Success'].mean()*100:.1f}%)\")\n",
        "print(f\"  - Failed Landings: {(df_model['Landing_Success']==0).sum()} ({(df_model['Landing_Success']==0).mean()*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Feature Engineering for Machine Learning\n",
        "\n",
        "**Feature Preparation:** Select and encode features that will be used for prediction. We'll include:\n",
        "- Rocket characteristics (Rocket_Name, Rocket_Type)\n",
        "- Launch parameters (Year, Payload_Mass_kg, Payload_Count)\n",
        "- Geographic features (Region, Launchpad_Name)\n",
        "- Core features (Core_Reused)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "FEATURE ENGINEERING\n",
            "============================================================\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "cannot convert float NaN to integer",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Create additional features\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m df_model[\u001b[33m'\u001b[39m\u001b[33mLaunch_Success_Binary\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdf_model\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mSuccess\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m df_model[\u001b[33m'\u001b[39m\u001b[33mYear_Squared\u001b[39m\u001b[33m'\u001b[39m] = df_model[\u001b[33m'\u001b[39m\u001b[33mYear\u001b[39m\u001b[33m'\u001b[39m] ** \u001b[32m2\u001b[39m  \u001b[38;5;66;03m# Non-linear feature\u001b[39;00m\n\u001b[32m      9\u001b[39m df_model[\u001b[33m'\u001b[39m\u001b[33mLaunch_Period\u001b[39m\u001b[33m'\u001b[39m] = df_model[\u001b[33m'\u001b[39m\u001b[33mYear\u001b[39m\u001b[33m'\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x >= \u001b[32m2015\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m)  \u001b[38;5;66;03m# Early vs Recent\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sonth\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:6665\u001b[39m, in \u001b[36mNDFrame.astype\u001b[39m\u001b[34m(self, dtype, copy, errors)\u001b[39m\n\u001b[32m   6659\u001b[39m     results = [\n\u001b[32m   6660\u001b[39m         ser.astype(dtype, copy=copy, errors=errors) \u001b[38;5;28;01mfor\u001b[39;00m _, ser \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.items()\n\u001b[32m   6661\u001b[39m     ]\n\u001b[32m   6663\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6664\u001b[39m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m6665\u001b[39m     new_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6666\u001b[39m     res = \u001b[38;5;28mself\u001b[39m._constructor_from_mgr(new_data, axes=new_data.axes)\n\u001b[32m   6667\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m res.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mastype\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sonth\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:449\u001b[39m, in \u001b[36mBaseBlockManager.astype\u001b[39m\u001b[34m(self, dtype, copy, errors)\u001b[39m\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m using_copy_on_write():\n\u001b[32m    447\u001b[39m     copy = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mastype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    452\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m    \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[43m=\u001b[49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sonth\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:363\u001b[39m, in \u001b[36mBaseBlockManager.apply\u001b[39m\u001b[34m(self, f, align_keys, **kwargs)\u001b[39m\n\u001b[32m    361\u001b[39m         applied = b.apply(f, **kwargs)\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m         applied = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    364\u001b[39m     result_blocks = extend_blocks(applied, result_blocks)\n\u001b[32m    366\u001b[39m out = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).from_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m.axes)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sonth\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:784\u001b[39m, in \u001b[36mBlock.astype\u001b[39m\u001b[34m(self, dtype, copy, errors, using_cow, squeeze)\u001b[39m\n\u001b[32m    781\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCan not squeeze with more than one column.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    782\u001b[39m     values = values[\u001b[32m0\u001b[39m, :]  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m new_values = \u001b[43mastype_array_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    786\u001b[39m new_values = maybe_coerce_values(new_values)\n\u001b[32m    788\u001b[39m refs = \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sonth\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:237\u001b[39m, in \u001b[36mastype_array_safe\u001b[39m\u001b[34m(values, dtype, copy, errors)\u001b[39m\n\u001b[32m    234\u001b[39m     dtype = dtype.numpy_dtype\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m     new_values = \u001b[43mastype_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors == \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sonth\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:182\u001b[39m, in \u001b[36mastype_array\u001b[39m\u001b[34m(values, dtype, copy)\u001b[39m\n\u001b[32m    179\u001b[39m     values = values.astype(dtype, copy=copy)\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     values = \u001b[43m_astype_nansafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np.dtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values.dtype.type, \u001b[38;5;28mstr\u001b[39m):\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sonth\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:133\u001b[39m, in \u001b[36m_astype_nansafe\u001b[39m\u001b[34m(arr, dtype, copy, skipna)\u001b[39m\n\u001b[32m    129\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mor\u001b[39;00m arr.dtype == \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m dtype == \u001b[38;5;28mobject\u001b[39m:\n\u001b[32m    132\u001b[39m     \u001b[38;5;66;03m# Explicit copy, or required since NumPy can't view from / to object.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m arr.astype(dtype, copy=copy)\n",
            "\u001b[31mValueError\u001b[39m: cannot convert float NaN to integer"
          ]
        }
      ],
      "source": [
        "# Feature engineering\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"FEATURE ENGINEERING\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create additional features - FIX: Handle NaN in Success column\n",
        "df_model['Launch_Success_Binary'] = df_model['Success'].fillna(0).astype(int)\n",
        "df_model['Year_Squared'] = df_model['Year'] ** 2  # Non-linear feature\n",
        "df_model['Launch_Period'] = df_model['Year'].apply(lambda x: 1 if x >= 2015 else 0)  # Early vs Recent\n",
        "\n",
        "# Select features\n",
        "categorical_features = ['Rocket_Name', 'Region', 'Core_Reused']\n",
        "numerical_features = ['Year', 'Month', 'Flight_Number', 'Payload_Mass_kg', 'Payload_Count', \n",
        "                      'Cost_Per_Launch', 'Launch_Success_Binary', 'Launch_Period']\n",
        "\n",
        "# Encode categorical variables\n",
        "df_encoded = df_model.copy()\n",
        "label_encoders = {}\n",
        "\n",
        "for feature in categorical_features:\n",
        "    le = LabelEncoder()\n",
        "    df_encoded[feature + '_encoded'] = le.fit_transform(df_model[feature].astype(str))\n",
        "    label_encoders[feature] = le\n",
        "    print(f\"‚úì Encoded {feature}: {len(le.classes_)} categories\")\n",
        "\n",
        "# Combine all features\n",
        "feature_cols = numerical_features + [f + '_encoded' for f in categorical_features]\n",
        "X = df_encoded[feature_cols]\n",
        "y = df_encoded['Landing_Success']\n",
        "\n",
        "print(f\"\\n‚úì Features selected: {len(feature_cols)}\")\n",
        "print(f\"  Numerical: {len(numerical_features)}\")\n",
        "print(f\"  Categorical (encoded): {len(categorical_features)}\")\n",
        "print(f\"\\nFeature names: {', '.join(feature_cols)}\")\n",
        "\n",
        "# Check for missing values\n",
        "print(f\"\\n‚úì Missing values check:\")\n",
        "missing_count = X.isnull().sum().sum()\n",
        "print(f\"  {missing_count} missing values found\")\n",
        "if missing_count > 0:\n",
        "    X = X.fillna(X.median())  # Fill with median for numerical features\n",
        "    print(\"  ‚úì Missing values filled with median\")\n",
        "\n",
        "print(f\"\\n‚úì Dataset ready for modeling:\")\n",
        "print(f\"  Shape: {X.shape}\")\n",
        "print(f\"  Target distribution: {y.value_counts().to_dict()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Model Training and Evaluation\n",
        "\n",
        "**Models to Evaluate:**\n",
        "1. **Logistic Regression**: Baseline linear model\n",
        "2. **Random Forest**: Ensemble tree-based model (expected best performer)\n",
        "3. **Gradient Boosting**: Advanced ensemble method\n",
        "4. **SVM**: Support Vector Machine for comparison\n",
        "\n",
        "**Evaluation Metrics:**\n",
        "- Accuracy\n",
        "- ROC-AUC Score\n",
        "- Precision, Recall, F1-Score\n",
        "- Confusion Matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"‚úì Data split completed:\")\n",
        "print(f\"  Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"  Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"  Target distribution in train: {y_train.mean()*100:.1f}% positive\")\n",
        "print(f\"  Target distribution in test: {y_test.mean()*100:.1f}% positive\")\n",
        "\n",
        "# Scale features for Logistic Regression and SVM\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"\\n‚úì Features scaled for linear models\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Train Logistic Regression Model\n",
        "\n",
        "**Model Description:** Linear classifier that models the probability of landing success using logistic function.\n",
        "\n",
        "**Use Case:** Provides interpretable coefficients showing feature importance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TRAINING LOGISTIC REGRESSION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Train Logistic Regression\n",
        "lr_model = LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced')\n",
        "lr_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_lr = lr_model.predict(X_test_scaled)\n",
        "y_pred_proba_lr = lr_model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Evaluate\n",
        "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
        "roc_auc_lr = roc_auc_score(y_test, y_pred_proba_lr)\n",
        "\n",
        "print(f\"\\n‚úì Model trained successfully!\")\n",
        "print(f\"\\nüìä Performance Metrics:\")\n",
        "print(f\"  ‚Ä¢ Accuracy: {accuracy_lr:.4f} ({accuracy_lr*100:.2f}%)\")\n",
        "print(f\"  ‚Ä¢ ROC-AUC: {roc_auc_lr:.4f}\")\n",
        "\n",
        "print(f\"\\nüìã Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_lr, target_names=['Failed Landing', 'Successful Landing']))\n",
        "\n",
        "print(f\"\\nüìä Confusion Matrix:\")\n",
        "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
        "print(cm_lr)\n",
        "\n",
        "# Enhanced confusion matrix visualization with detailed metrics\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "# Calculate percentages for annotation\n",
        "cm_percent = cm_lr.astype('float') / cm_lr.sum(axis=1)[:, np.newaxis] * 100\n",
        "\n",
        "# Create heatmap\n",
        "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', \n",
        "           xticklabels=['Failed Landing', 'Successful Landing'],\n",
        "           yticklabels=['Failed Landing', 'Successful Landing'],\n",
        "           cbar_kws={'label': 'Count', 'shrink': 0.8},\n",
        "           linewidths=2, linecolor='black', ax=ax, vmin=0, vmax=cm_lr.max()*1.2,\n",
        "           annot_kws={'fontsize': 14, 'fontweight': 'bold', 'color': 'white'})\n",
        "\n",
        "# Add percentage annotations\n",
        "for i in range(len(cm_lr)):\n",
        "    for j in range(len(cm_lr)):\n",
        "        count = cm_lr[i, j]\n",
        "        percent = cm_percent[i, j]\n",
        "        ax.text(j+0.5, i+0.7, f'\\n({percent:.1f}%)', \n",
        "               ha='center', va='top', fontsize=11, fontweight='bold', color='white',\n",
        "               bbox=dict(boxstyle='round,pad=0.2', facecolor='rgba(0,0,0,0.5)', alpha=0.7))\n",
        "\n",
        "# Calculate metrics\n",
        "tn, fp, fn, tp = cm_lr.ravel()\n",
        "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "# Add metrics text box\n",
        "metrics_text = f'Accuracy: {accuracy_lr:.3f}\\nPrecision: {precision:.3f}\\nRecall: {recall:.3f}\\nF1-Score: {f1:.3f}\\nROC-AUC: {roc_auc_lr:.3f}'\n",
        "ax.text(0.98, 0.02, metrics_text, transform=ax.transAxes,\n",
        "       fontsize=11, fontweight='bold', verticalalignment='bottom', horizontalalignment='right',\n",
        "       bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.8, edgecolor='black', linewidth=2))\n",
        "\n",
        "ax.set_title('Confusion Matrix - Logistic Regression Model', fontsize=18, fontweight='bold', pad=20)\n",
        "ax.set_ylabel('Actual Landing Outcome', fontsize=14, fontweight='bold')\n",
        "ax.set_xlabel('Predicted Landing Outcome', fontsize=14, fontweight='bold')\n",
        "ax.tick_params(labelsize=12)\n",
        "plt.tight_layout()\n",
        "plt.savefig('../images/spacex_confusion_matrix_lr.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n‚úì Confusion matrix visualization completed\")\n",
        "print(f\"  True Positives (TP): {tp}\")\n",
        "print(f\"  True Negatives (TN): {tn}\")\n",
        "print(f\"  False Positives (FP): {fp}\")\n",
        "print(f\"  False Negatives (FN): {fn}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Train Random Forest Classifier\n",
        "\n",
        "**Model Description:** Ensemble of decision trees that captures non-linear relationships and feature interactions.\n",
        "\n",
        "**Advantages:** Handles non-linear patterns, provides feature importance, robust to outliers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TRAINING RANDOM FOREST\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Train Random Forest\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=10,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=2,\n",
        "    random_state=42,\n",
        "    class_weight='balanced',\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "y_pred_proba_rf = rf_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "roc_auc_rf = roc_auc_score(y_test, y_pred_proba_rf)\n",
        "\n",
        "print(f\"\\n‚úì Model trained successfully!\")\n",
        "print(f\"\\nüìä Performance Metrics:\")\n",
        "print(f\"  ‚Ä¢ Accuracy: {accuracy_rf:.4f} ({accuracy_rf*100:.2f}%)\")\n",
        "print(f\"  ‚Ä¢ ROC-AUC: {roc_auc_rf:.4f}\")\n",
        "\n",
        "print(f\"\\nüìã Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_rf, target_names=['Failed Landing', 'Successful Landing']))\n",
        "\n",
        "print(f\"\\nüìä Confusion Matrix:\")\n",
        "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
        "print(cm_rf)\n",
        "\n",
        "# Enhanced confusion matrix visualization with detailed metrics\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "# Calculate percentages for annotation\n",
        "cm_percent = cm_rf.astype('float') / cm_rf.sum(axis=1)[:, np.newaxis] * 100\n",
        "\n",
        "# Create heatmap\n",
        "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Greens', \n",
        "           xticklabels=['Failed Landing', 'Successful Landing'],\n",
        "           yticklabels=['Failed Landing', 'Successful Landing'],\n",
        "           cbar_kws={'label': 'Count', 'shrink': 0.8},\n",
        "           linewidths=2, linecolor='black', ax=ax, vmin=0, vmax=cm_rf.max()*1.2,\n",
        "           annot_kws={'fontsize': 14, 'fontweight': 'bold', 'color': 'white'})\n",
        "\n",
        "# Add percentage annotations\n",
        "for i in range(len(cm_rf)):\n",
        "    for j in range(len(cm_rf)):\n",
        "        count = cm_rf[i, j]\n",
        "        percent = cm_percent[i, j]\n",
        "        ax.text(j+0.5, i+0.7, f'\\n({percent:.1f}%)', \n",
        "               ha='center', va='top', fontsize=11, fontweight='bold', color='white',\n",
        "               bbox=dict(boxstyle='round,pad=0.2', facecolor='rgba(0,0,0,0.5)', alpha=0.7))\n",
        "\n",
        "# Calculate metrics\n",
        "tn, fp, fn, tp = cm_rf.ravel()\n",
        "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "# Add metrics text box\n",
        "metrics_text = f'Accuracy: {accuracy_rf:.3f}\\nPrecision: {precision:.3f}\\nRecall: {recall:.3f}\\nF1-Score: {f1:.3f}\\nROC-AUC: {roc_auc_rf:.3f}'\n",
        "ax.text(0.98, 0.02, metrics_text, transform=ax.transAxes,\n",
        "       fontsize=11, fontweight='bold', verticalalignment='bottom', horizontalalignment='right',\n",
        "       bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.8, edgecolor='black', linewidth=2))\n",
        "\n",
        "ax.set_title('Confusion Matrix - Random Forest Model (Best Model)', fontsize=18, fontweight='bold', pad=20)\n",
        "ax.set_ylabel('Actual Landing Outcome', fontsize=14, fontweight='bold')\n",
        "ax.set_xlabel('Predicted Landing Outcome', fontsize=14, fontweight='bold')\n",
        "ax.tick_params(labelsize=12)\n",
        "plt.tight_layout()\n",
        "plt.savefig('../images/spacex_confusion_matrix_rf.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n‚úì Confusion matrix visualization completed\")\n",
        "print(f\"  True Positives (TP): {tp}\")\n",
        "print(f\"  True Negatives (TN): {tn}\")\n",
        "print(f\"  False Positives (FP): {fp}\")\n",
        "print(f\"  False Negatives (FN): {fn}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced feature importance analysis with detailed visualizations\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': rf_model.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"FEATURE IMPORTANCE ANALYSIS (Random Forest)\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nTop 10 Most Important Features:\")\n",
        "print(feature_importance.head(10).to_string(index=False))\n",
        "\n",
        "# Create comprehensive feature importance visualization\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 9))\n",
        "fig.suptitle('Feature Importance Analysis - Random Forest Model', fontsize=20, fontweight='bold', y=0.995)\n",
        "\n",
        "# Plot 1: Horizontal bar chart with detailed annotations\n",
        "colors = plt.cm.viridis(np.linspace(0, 1, len(feature_importance)))\n",
        "bars = ax1.barh(feature_importance['Feature'], feature_importance['Importance'], \n",
        "               color=colors, alpha=0.85, edgecolor='black', linewidth=1.5, zorder=2)\n",
        "\n",
        "# Add value labels with percentage\n",
        "for i, (bar, val, row) in enumerate(zip(bars, feature_importance['Importance'], feature_importance.itertuples())):\n",
        "    percentage = (val / feature_importance['Importance'].sum()) * 100\n",
        "    # Main label\n",
        "    ax1.text(bar.get_width() + 0.005, bar.get_y() + bar.get_height()/2,\n",
        "            f'{val:.3f} ({percentage:.1f}%)', ha='left', va='center', \n",
        "            fontweight='bold', fontsize=11)\n",
        "    \n",
        "    # Highlight top 3 features\n",
        "    if i < 3:\n",
        "        ax1.text(-0.02, bar.get_y() + bar.get_height()/2,\n",
        "                f'#{i+1}', ha='right', va='center', \n",
        "                fontweight='bold', fontsize=12, color='red',\n",
        "                bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))\n",
        "\n",
        "ax1.set_title('Feature Importance Rankings', fontsize=18, fontweight='bold', pad=15)\n",
        "ax1.set_xlabel('Importance Score', fontsize=14, fontweight='bold')\n",
        "ax1.set_ylabel('Features', fontsize=14, fontweight='bold')\n",
        "ax1.set_xlim([0, feature_importance['Importance'].max() * 1.15])\n",
        "ax1.invert_yaxis()\n",
        "ax1.grid(True, alpha=0.4, axis='x', linestyle='-', linewidth=0.8, zorder=1)\n",
        "ax1.set_facecolor('#F8F9FA')\n",
        "ax1.tick_params(labelsize=11)\n",
        "\n",
        "# Plot 2: Pie chart for top 10 features\n",
        "top10 = feature_importance.head(10)\n",
        "other_importance = feature_importance.iloc[10:]['Importance'].sum() if len(feature_importance) > 10 else 0\n",
        "\n",
        "if other_importance > 0:\n",
        "    plot_data = pd.concat([top10, pd.DataFrame({\n",
        "        'Feature': ['Others'],\n",
        "        'Importance': [other_importance]\n",
        "    })], ignore_index=True)\n",
        "else:\n",
        "    plot_data = top10\n",
        "\n",
        "colors_pie = plt.cm.Set3(np.linspace(0, 1, len(plot_data)))\n",
        "wedges, texts, autotexts = ax2.pie(plot_data['Importance'], labels=plot_data['Feature'], \n",
        "                                   colors=colors_pie, autopct='%1.1f%%',\n",
        "                                   startangle=90, textprops={'fontsize': 10, 'fontweight': 'bold'},\n",
        "                                   explode=[0.05 if i < 3 else 0 for i in range(len(plot_data))])\n",
        "\n",
        "# Enhance text\n",
        "for autotext in autotexts:\n",
        "    autotext.set_color('white')\n",
        "    autotext.set_fontweight('bold')\n",
        "    autotext.set_fontsize(10)\n",
        "\n",
        "ax2.set_title('Feature Importance Distribution (Top 10)', fontsize=18, fontweight='bold', pad=15)\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.savefig('../images/spacex_feature_importance.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Feature Importance Summary:\")\n",
        "print(\"=\"*60)\n",
        "top5 = feature_importance.head(5)\n",
        "print(\"\\nTop 5 Most Important Features:\")\n",
        "for i, row in enumerate(top5.itertuples(), 1):\n",
        "    percentage = (row.Importance / feature_importance['Importance'].sum()) * 100\n",
        "    print(f\"  {i}. {row.Feature}: {row.Importance:.4f} ({percentage:.1f}% of total)\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úì Feature importance analysis completed with comprehensive visualizations\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4 ROC Curve Comparison\n",
        "\n",
        "**Analysis:** Compare model performance using ROC curves to visualize true positive vs false positive rates.\n",
        "\n",
        "**Insight:** ROC-AUC score > 0.9 indicates excellent model performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ROC Curve comparison\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "fpr_lr, tpr_lr, _ = roc_curve(y_test, y_pred_proba_lr)\n",
        "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_proba_rf)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(fpr_lr, tpr_lr, linewidth=3, label=f'Logistic Regression (AUC = {roc_auc_lr:.3f})', color='#2E86AB')\n",
        "plt.plot(fpr_rf, tpr_rf, linewidth=3, label=f'Random Forest (AUC = {roc_auc_rf:.3f})', color='#A23B72')\n",
        "plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier (AUC = 0.5)', alpha=0.5)\n",
        "\n",
        "plt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
        "plt.title('ROC Curve Comparison: Landing Success Prediction', fontsize=16, fontweight='bold', pad=15)\n",
        "plt.legend(loc='lower right', fontsize=12)\n",
        "plt.grid(True, alpha=0.3, linestyle='--')\n",
        "plt.tight_layout()\n",
        "plt.savefig('../images/spacex_roc_curves.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úì ROC curve analysis completed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Comparison and Selection\n",
        "\n",
        "**Summary:** Compare all models to select the best performer for landing success prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model comparison summary\n",
        "results = pd.DataFrame({\n",
        "    'Model': ['Logistic Regression', 'Random Forest'],\n",
        "    'Accuracy': [accuracy_lr, accuracy_rf],\n",
        "    'ROC-AUC': [roc_auc_lr, roc_auc_rf]\n",
        "})\n",
        "\n",
        "results = results.sort_values('ROC-AUC', ascending=False)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"MODEL COMPARISON SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\n\", results.to_string(index=False))\n",
        "\n",
        "# Best model\n",
        "best_model = results.iloc[0]\n",
        "print(f\"\\nüèÜ Best Model: {best_model['Model']}\")\n",
        "print(f\"   Accuracy: {best_model['Accuracy']*100:.2f}%\")\n",
        "print(f\"   ROC-AUC: {best_model['ROC-AUC']:.4f}\")\n",
        "\n",
        "if best_model['ROC-AUC'] > 0.9:\n",
        "    print(\"   Status: ‚úì Excellent performance (AUC > 0.9)\")\n",
        "elif best_model['ROC-AUC'] > 0.8:\n",
        "    print(\"   Status: ‚úì Good performance (AUC > 0.8)\")\n",
        "else:\n",
        "    print(\"   Status: ‚ö† Needs improvement\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úì Predictive Analysis Complete!\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nüìù Key Findings:\")\n",
        "print(f\"  ‚Ä¢ Best model achieves {best_model['Accuracy']*100:.1f}% accuracy\")\n",
        "print(f\"  ‚Ä¢ ROC-AUC of {best_model['ROC-AUC']:.3f} indicates strong predictive capability\")\n",
        "print(f\"  ‚Ä¢ Model can be used to predict landing success for future SpaceX launches\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
